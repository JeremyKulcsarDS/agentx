{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) Tool\n",
    "\n",
    "The ability to retrieve relevant context before giving an answer provides huge performance boost to LLM performance. Here shows how you can build a simple tool that retrieve information from pdf files under the same folder.\n",
    "\n",
    "RAG can be broken down into 4 steps.\n",
    "1. Input is converted into the `input_embedding` vector.\n",
    "2. The `input_embedding` vector is used to retrieve relevant documents by calculating the distance between the `input_embedding` vector and each `document_embedding` vector.\n",
    "    2.1 This process is optionally accelerated by creating an index, which is what most vector databases manage for you.\n",
    "3. The relevant documents (those that have a close distance with the `input_embedding` vector) are returned alongside its metadata.\n",
    "4. The returned documents are appended to the LLM prompt.\n",
    "\n",
    "In the above example, only sessions that are semantically different (i.e. the Analyst Report) are used to create the `document_embedding` vector. Adding non natural language data to construct the embedding vector could result in lower performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha256\n",
    "import os\n",
    "# XEntropy will send a Webhook-Secret header to your endpoint for authentication.\n",
    "webhook_secret = sha256(b'YOUR OWN SEED').hexdigest()\n",
    "\n",
    "# Replace with your own \n",
    "env = {\n",
    "    \"WEBHOOK_SECRET\":webhook_secret,\n",
    "    'AZURE_OPENAI_API_BASE':os.environ.get('AZURE_OPENAI_API_BASE'), # YOUR AZURE_OPENAI_API_BASE\n",
    "    'AZURE_OPENAI_API_KEY':os.environ.get('AZURE_OPENAI_API_KEY') # YOUR AZURE_OPENAI_API_KEY\n",
    "}\n",
    "\n",
    "with open('.env', 'w') as f:\n",
    "    f.write('\\n'.join([f'{key}={value}' for key, value in env.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We will download some etf analyst reports to demonstrate RAG\n",
    "etf_dataframe = pandas.read_csv('rag-tool/etf_list.csv', sep='\\t')\n",
    "for etf in tqdm(etf_dataframe.Symbol.values):\n",
    "    with open(f'rag-tool/pdfs/{etf}.pdf', 'wb') as f:\n",
    "        response = requests.get(f'https://etfdb.com/advisor_reports/{etf}/')\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "# Take a look at the extracted text\n",
    "reader = PdfReader(\"rag-tool/pdfs/QQQ.pdf\")\n",
    "page = reader.pages[0]\n",
    "print(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the text under the Analyst Report session to create the embedding.\n",
    "def extract_full_text(path):\n",
    "    reader = PdfReader(path)\n",
    "    page = reader.pages[0]\n",
    "    return page.extract_text()\n",
    "\n",
    "\n",
    "def extract_analyst_report(text):\n",
    "    return text.split('Analyst Report')[1].split('Performance Data')[0]\n",
    "\n",
    "\n",
    "etf_dataframe['Full Text'] = etf_dataframe.Symbol.apply(\n",
    "    lambda x: extract_full_text(f'rag-tool/pdfs/{x}.pdf'))\n",
    "etf_dataframe['Analyst Report'] = etf_dataframe['Full Text'].apply(\n",
    "    lambda x: extract_analyst_report(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert natural language into embeddings\n",
    "# https://docs.trychroma.com/embeddings details multiple method of doing so.\n",
    "# In this tutorial we will continue with Azure OpenAI Ada 2.\n",
    "\n",
    "def ada2_embedding(text):\n",
    "    azure_base = os.environ.get('AZURE_OPENAI_API_BASE')\n",
    "    response = requests.post(\n",
    "        f'{azure_base}/openai/deployments/ada2/embeddings?api-version=2023-07-01-preview',\n",
    "        json={\n",
    "            \"input\": text\n",
    "        },\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"api-key\": os.environ.get('AZURE_OPENAI_API_KEY')\n",
    "        }\n",
    "    ).json()\n",
    "    return response.get('data')[0].get('embedding')\n",
    "\n",
    "\n",
    "etf_dataframe['embedding'] = etf_dataframe['Analyst Report'].apply(\n",
    "    ada2_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to parquet\n",
    "# it will be loaded to ChromaDB in server.py\n",
    "etf_dataframe.to_parquet('rag-tool/etf.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following to start a FastAPI server on your virtual machine. As vector database can consume large amount of memory, when under resource constrain you can consider using managed vector database on the cloud.\n",
    "```bash\n",
    "cd rag-tool\n",
    "uvicorn server:app --host 0.0.0.0 --port 80 --reload\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the server is working\n",
    "url = f'http://IP_OF_YOUR_VIRTUAL_MACHINE/retrieve'\n",
    "response = requests.post(\n",
    "    url,\n",
    "    json={\n",
    "        'etf_description': 'Focus investment on semiconductor stock.'\n",
    "    },\n",
    "    headers={\n",
    "        'Webhook-Secret': webhook_secret\n",
    "    }\n",
    ")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siumai.tool import Tool\n",
    "from pydantic import BaseModel\n",
    "# publish the tool to XEntropy\n",
    "class ETFDescription(BaseModel):\n",
    "    etf_description: str\n",
    "\n",
    "\n",
    "etf_search = Tool(\n",
    "    api_key=os.environ.get('XENTROPY_API_KEY'),\n",
    "    name='etf_search',\n",
    "    description='Search for ETFs that satisfy the given description.',\n",
    "    endpoint=url,\n",
    "    input_model=ETFDescription,\n",
    "    price=1,  # xentropy_credit per request. 0 means free to use. 1 USD = 100,000 xentropy_credit\n",
    "    free_quota=100,  # number of free uses per user per day\n",
    ")\n",
    "\n",
    "tool_upload = etf_search.publish(\n",
    "    webhook_secret=webhook_secret,\n",
    "    # set to True, if you want the tool to be searchable and usable by other users on XEntropy.\n",
    "    public=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the tool works\n",
    "etf_search = Tool.load(\n",
    "    tool_upload[\"name\"],\n",
    "    api_key=os.environ.get('XENTROPY_API_KEY'))\n",
    "# you are not charged for using your own tool\n",
    "etf_search.run(etf_description='Focus investment on semiconductor stock.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try equipping the tool to an agents and check its effects\n",
    "import autogen\n",
    "llm_config = {\n",
    "    \"functions\": [\n",
    "        # added the new tool here\n",
    "        {\n",
    "            \"name\": etf_search.name,\n",
    "            \"description\": etf_search.description,\n",
    "            \"parameters\": etf_search.input_model_schema(),\n",
    "        },\n",
    "    ],\n",
    "    \"config_list\": autogen.config_list_from_models(model_list=['gpt-35']),\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "# Construct the agent with the new config\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"Use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") == \"\"\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\n",
    "        \"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=2,\n",
    ")\n",
    "\n",
    "# Register the new functions to the user_proxy\n",
    "user_proxy.register_function(\n",
    "    {\n",
    "        etf_search.name: etf_search.run,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    chatbot,\n",
    "    message=\"Suggest me ETFs to gain exposure in semiconductors.\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A* Chat\n",
    "\n",
    "As your application grows bigger, you may want to create more than one agent, each with a different set of tools to handle different part of the problem. Orchestrating a conversation between these agents can be challenging, especially when it is difficult to determind which agent should be called at each timestep.\n",
    "\n",
    "Seeing multi-agent chat as a path finding problem, where each message in a message history is analogous to a node, and the message history as the path, A* algorithm can be used to find the optimal path to the end of the conversation.\n",
    "\n",
    "A* algorithm is a path finding algorithm that is widely used. It is a variant of Dijkstra's algorithm, which is used to find the shortest path between two nodes in a graph. A* algorithm is an extension of Dijkstra's algorithm, which adds a heuristic function to guide the search towards the goal. The heuristic function is an estimation of the distance between the current node and the goal. The algorithm will always choose the node with the lowest cost, which is the sum of the distance from the start node to the current node and the heuristic function.\n",
    "\n",
    "Using A* Chat, instead of having to manually program agent behaviours, you can simply define the heuristic function that estimates how *close* the message history to the goal, and the algorithm will automatically orchestrate the conversation between the agents to reach the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 0: A* Chat to write test problems and solutions according to Bloom’s Taxonomy of Educational Objectives\n",
    "We will use A* Chat to orchestrate a conversation between a two agents:\n",
    "- test_giver: writes a set of questions and marking scheme about a subject\n",
    "- bloom: apply [Bloom’s Taxonomy of Educational Objectives](https://en.wikipedia.org/wiki/Bloom%27s_taxonomy#:~:text=Bloom's%20taxonomy%20is%20a%20set,cognitive%2C%20affective%20and%20psychomotor%20domains.) to evaluate the quality of the test problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Function\nparameters\n  Input should be a valid dictionary [type=dict_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.5/v/dict_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 92\u001b[0m\n\u001b[1;32m     81\u001b[0m bloom_scoring_tools \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     82\u001b[0m     BloomScorer(\n\u001b[1;32m     83\u001b[0m         bloom,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m bloom_objective \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknowledge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomprehension\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynthesis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     89\u001b[0m ]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# this agent will review the question / answer pair\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m reviewer \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreviewer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[38;5;124;43mUse the tool you have been provided to review the question. Critically access if the test question is at the right level and quality of Bloom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms Taxonomy. If not, give feedback on how to improve the question.\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbloom_scoring_tools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# At each timestep, A* minimize heuristic + cost\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# heuristic: an estimation of the distance between the current state and the goal state\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# cost: the distance between the start state and the current state\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Heuristic is the sum of the difference between the current Bloom score and the target Bloom score\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBloomReport\u001b[39;00m(BaseModel):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/agentx/agent.py:52\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, name, generation_config, system_prompt, tools, termination_function, reduce_function)\u001b[0m\n\u001b[1;32m     50\u001b[0m _generation_config \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmodel_copy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     _generation_config\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     53\u001b[0m         tool\u001b[38;5;241m.\u001b[39mname:Function(\n\u001b[1;32m     54\u001b[0m             name\u001b[38;5;241m=\u001b[39mtool\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     55\u001b[0m             description\u001b[38;5;241m=\u001b[39mtool\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m     56\u001b[0m             parameters\u001b[38;5;241m=\u001b[39mtool\u001b[38;5;241m.\u001b[39minput_json_schema,\n\u001b[1;32m     57\u001b[0m         ) \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools\n\u001b[1;32m     58\u001b[0m     }\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt \u001b[38;5;241m=\u001b[39m system_prompt\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/agentx/agent.py:53\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m _generation_config \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmodel_copy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     _generation_config\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 53\u001b[0m         tool\u001b[38;5;241m.\u001b[39mname:\u001b[43mFunction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_json_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools\n\u001b[1;32m     58\u001b[0m     }\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt \u001b[38;5;241m=\u001b[39m system_prompt\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pydantic/main.py:164\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    163\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m \u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Function\nparameters\n  Input should be a valid dictionary [type=dict_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.5/v/dict_type"
     ]
    }
   ],
   "source": [
    "from agentx.agent import Agent\n",
    "from agentx.schema import GenerationConfig, Message, Content\n",
    "from agentx.groupchat import astar_chat, reconstruct_path\n",
    "from agentx.tool import Tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Literal\n",
    "from functools import partial\n",
    "from dotenv import load_dotenv\n",
    "from rich import print as rich_print\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    api_type='azure',\n",
    "    api_key=os.environ.get('AZURE_OPENAI_KEY'),\n",
    "    base_url=os.environ.get('AZURE_OPENAI_ENDPOINT'),\n",
    "    azure_deployment='gpt-35',\n",
    ")\n",
    "\n",
    "# this agent will write the question / answer pair\n",
    "test_writer = Agent(\n",
    "    name='test_writer',\n",
    "    generation_config=generation_config,\n",
    "    system_prompt='''According to the user request and feedback, write a test question / answer pair.''',\n",
    ")\n",
    "\n",
    "class TestQuestion(BaseModel):\n",
    "    question:str\n",
    "    answer:str\n",
    "\n",
    "# Define agents and tools for reviewing the test question\n",
    "bloom = Agent(\n",
    "    name='bloom_expert',\n",
    "    generation_config=generation_config,\n",
    "    system_prompt=\"You are an education expert and highly knowledgeable about Bloom's taxonomy of education objective.\",\n",
    ")\n",
    "\n",
    "class BloomScore(BaseModel):\n",
    "    score:float = Field(0, ge=0, le=10),\n",
    "    improvement_suggestion:str\n",
    "    objective:Literal['knowledge', 'comprehension', 'application', 'analysis', 'synthesis', 'evaluation']\n",
    "\n",
    "class BloomScorer(Tool):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent:Agent,\n",
    "        bloom_objective:Literal['knowledge', 'comprehension', 'application', 'analysis', 'synthesis', 'evaluation'],\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.agent = agent\n",
    "        self.bloom_objective = bloom_objective\n",
    "\n",
    "    def run(self, **kwargs) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    async def a_run(self, **kwargs) -> str:\n",
    "        test_question = self.input_model(**kwargs)\n",
    "        response = await self.agent.a_generate_response(\n",
    "            messages=[\n",
    "                Message(\n",
    "                    role='user',\n",
    "                    content=Content(\n",
    "                        text='''Please give a score of 10 to represent the test question's quality at the {bloom_objective} level of Bloom's Taxonomy.\n",
    "The test question:\n",
    "{test_question}\n",
    "\n",
    "You must reply an JSON object.'''.format(\n",
    "    bloom_objective=self.bloom_objective,\n",
    "    test_question=test_question.model_dump()\n",
    "),\n",
    "                    ),\n",
    "                )\n",
    "            ],\n",
    "            output_model=BloomScore\n",
    "        )\n",
    "        return response.content.text\n",
    "\n",
    "bloom_scoring_tools = [\n",
    "    BloomScorer(\n",
    "        bloom,\n",
    "        bloom_objective,\n",
    "        name='{bloom_objective}_scorer'.format(bloom_objective=bloom_objective),\n",
    "        description='''Give a score of 10 to represent that the test question is at the {bloom_objective} level of Bloom's Taxonomy and the question's quality.'''.format(bloom_objective=bloom_objective),\n",
    "        input_model=TestQuestion,\n",
    "    ) for bloom_objective in ['knowledge', 'comprehension', 'application', 'analysis', 'synthesis', 'evaluation']\n",
    "]\n",
    "\n",
    "# this agent will review the question / answer pair\n",
    "reviewer = Agent(\n",
    "    name='reviewer',\n",
    "    generation_config=generation_config,\n",
    "    system_prompt='''Use the tool you have been provided to review the question. Critically access if the test question is at the right level and quality of Bloom's Taxonomy. If not, give feedback on how to improve the question.''',\n",
    "    tools=bloom_scoring_tools,\n",
    ")\n",
    "\n",
    "# At each timestep, A* minimize heuristic + cost\n",
    "# heuristic: an estimation of the distance between the current state and the goal state\n",
    "# cost: the distance between the start state and the current state\n",
    "\n",
    "# Heuristic is the sum of the difference between the current Bloom score and the target Bloom score\n",
    "class BloomReport(BaseModel):\n",
    "    knowledge:float = Field(0, ge=0, le=10)\n",
    "    comprehension:float = Field(0, ge=0, le=10)\n",
    "    application:float = Field(0, ge=0, le=10)\n",
    "    analysis:float = Field(0, ge=0, le=10)\n",
    "    synthesis:float = Field(0, ge=0, le=10)\n",
    "    evaluation:float = Field(0, ge=0, le=10)\n",
    "\n",
    "extractor = Agent(\n",
    "    name='extractor',\n",
    "    generation_config=generation_config,\n",
    "    system_prompt='''Extract the latest Bloom score from the messages history. You must reply an JSON object.''',\n",
    ")\n",
    "\n",
    "def heuristic(\n",
    "    messages:List[Message], \n",
    "    target:Dict[Literal['knowledge', 'comprehension', 'application', 'analysis', 'synthesis', 'evaluation'], float]\n",
    ") -> float:\n",
    "    if 'test_writer' not in [message.name for message in messages]:\n",
    "        # no test question has been written\n",
    "        return 10\n",
    "    if 'reviewer' not in [message.name for message in messages]:\n",
    "        # no review has been made\n",
    "        return 10\n",
    "    if not messages[-1].name in [f'{bloom_objective}_scorer' for bloom_objective in ['knowledge', 'comprehension', 'application', 'analysis', 'synthesis', 'evaluation']]:\n",
    "        # heuristic only applies when the last message is a Bloom scorer\n",
    "        return None\n",
    "    \n",
    "    bloom_report = extractor.generate_response(\n",
    "        messages=[\n",
    "            Message(\n",
    "                role='user',\n",
    "                content=Content(\n",
    "                    text='Based on this chat history: {history}'.format(\n",
    "                        history=[message.model_dump_json(\n",
    "                            exclude_unset=True,\n",
    "                            exclude_none=True\n",
    "                        ) for message in messages]\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        ] + [\n",
    "            Message(\n",
    "                role='user',\n",
    "                content=Content(\n",
    "                    text='Extract the latest Bloom scores. You must reply an JSON object.'),\n",
    "            ),\n",
    "        ],\n",
    "        output_model=BloomReport,\n",
    "    ).content.text\n",
    "    bloom_report = BloomReport.model_validate_json(bloom_report).model_dump()\n",
    "    \n",
    "    # print out for easier debugging and illustration\n",
    "    rich_print([message for message in messages if message.name=='test_writer'][-1].content.text)\n",
    "    rich_print(bloom_report)\n",
    "\n",
    "    difference = sum(\n",
    "        [\n",
    "            abs(bloom_report[objective] - target[objective]) for objective in ['knowledge', 'comprehension', 'application', 'analysis', 'synthesis', 'evaluation']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return difference / sum(target.values()) * 10\n",
    "\n",
    "# Cost is the number of LLM calls\n",
    "def cost(messages:List[Message], next_message:List[Message]) -> float:\n",
    "    cost = len(next_message)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:04<00:19,  2.45s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'model_validate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m target \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknowledge\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomprehension\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynthesis\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m      4\u001b[0m init_message \u001b[38;5;241m=\u001b[39m Message(\n\u001b[1;32m      5\u001b[0m     role \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     content \u001b[38;5;241m=\u001b[39m Content(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m reconstructed_path, came_from, cost_so_far, heuristic_map, hash_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m astar_chat(\n\u001b[1;32m     14\u001b[0m     agents \u001b[38;5;241m=\u001b[39m [test_writer, reviewer],\n\u001b[1;32m     15\u001b[0m     heuristic \u001b[38;5;241m=\u001b[39m partial(heuristic, target\u001b[38;5;241m=\u001b[39mtarget),\n\u001b[1;32m     16\u001b[0m     cost \u001b[38;5;241m=\u001b[39m cost,\n\u001b[1;32m     17\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [init_message],\n\u001b[1;32m     18\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     19\u001b[0m     n_replies \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     20\u001b[0m     max_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/agentx/groupchat.py:106\u001b[0m, in \u001b[0;36mastar_chat\u001b[0;34m(agents, messages, cost, heuristic, threshold, n_replies, max_iteration)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Flatten the list of tasks\u001b[39;00m\n\u001b[1;32m    104\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m tasks \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m--> 106\u001b[0m generated_messages:List[Union[Message, List[Message], \u001b[38;5;28;01mNone\u001b[39;00m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    107\u001b[0m generated_messages:List[Union[Message, List[Message]]] \u001b[38;5;241m=\u001b[39m [message \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m generated_messages \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    108\u001b[0m generated_messages:List[List[Message]] \u001b[38;5;241m=\u001b[39m [message \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, List) \u001b[38;5;28;01melse\u001b[39;00m [message] \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m generated_messages]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/agentx/agent.py:194\u001b[0m, in \u001b[0;36mAgent.a_generate_response\u001b[0;34m(self, messages, output_model)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool_calls \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_function_map\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    188\u001b[0m             tool_call\u001b[38;5;241m.\u001b[39mfunction_call\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m         ) \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m tool_calls \u001b[38;5;28;01mif\u001b[39;00m tool_call\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    192\u001b[0m     ]\n\u001b[0;32m--> 194\u001b[0m     responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    196\u001b[0m     tool_responses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    197\u001b[0m     multimodal_responses \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/agentx/tool.py:173\u001b[0m, in \u001b[0;36mTool.a_run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApi-Key\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParent-Id\u001b[39m\u001b[38;5;124m'\u001b[39m: parent_id,\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    170\u001b[0m }\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# use the tool\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m(kwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# use the tool asynchronously\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'model_validate'"
     ]
    }
   ],
   "source": [
    "\n",
    "# notice how the target is iteratively reached\n",
    "\n",
    "target = {'knowledge': 10, 'comprehension': 5, 'application': 0, 'analysis': 0, 'synthesis': 0, 'evaluation': 0}\n",
    "init_message = Message(\n",
    "    role = 'user',\n",
    "    content = Content(\n",
    "        text = '''According to the target {target}, write a multiple choice test question / answer pair about Mendelian Genetics'''.format(\n",
    "            target=target\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "reconstructed_path, came_from, cost_so_far, heuristic_map, hash_map = await astar_chat(\n",
    "    agents = [test_writer, reviewer],\n",
    "    heuristic = partial(heuristic, target=target),\n",
    "    cost = cost,\n",
    "    messages = [init_message],\n",
    "    threshold = 2,\n",
    "    n_replies = 1,\n",
    "    max_iteration = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
